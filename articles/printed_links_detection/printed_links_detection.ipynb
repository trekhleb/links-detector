{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom_objects_detector_v5__public.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fsJzz0oPI3gM",
        "V8x67CQmIaRG",
        "Prjfd9CeM-w0"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C6RmYsKrEvV"
      },
      "source": [
        "# ðŸ“– ðŸ‘†ðŸ» Printed Links Detection Using TensorFlow 2 Object Detection API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNeiUTuPa-K7"
      },
      "source": [
        "![Links Detector Cover](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/01-banner.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIkPgKIWsHiO"
      },
      "source": [
        "## ðŸ“ƒ TL;DR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJedZpEkSKYq"
      },
      "source": [
        "_In this article we will start solving the issue of making the printed links (i.e. in a book or in a magazine) clickable via your smartphone camera._\n",
        "\n",
        "We will use TensorFlow 2 [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) to train a custom object detector model to find positions and bounding boxes of the sub-strings like `https://` in the text image (i.e. in smartphone camera stream).\n",
        "\n",
        "The text of each link (right continuation of `https://` bounding box) will be recognized by using [Tesseract](https://tesseract.projectnaptha.com/) library. The recognition part will not be covered in this article but you may find the complete code example of the application in [links-detector repository](https://github.com/trekhleb/links-detector).   \n",
        "\n",
        "> ðŸš€ [**Launch Links Detector demo**](https://trekhleb.github.io/links-detector/) from your smartphone to see the final result.\n",
        "\n",
        "> ðŸ“ [**Open links-detector repository**](https://github.com/trekhleb/links-detector) on GitHub to see the complete source code of the application.\n",
        "\n",
        "Here is how the final solution will look like:\n",
        "\n",
        "![Links Detector Demo](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/03-links-detector-demo.gif)\n",
        "\n",
        "> âš ï¸ Currently the application is in _experimental_ _Alpha_ stage and has [many issues and limitations](https://github.com/trekhleb/links-detector/issues?q=is%3Aopen+is%3Aissue+label%3Aenhancement). So don't raise your expectations bar to high until these issues are resolved ðŸ¤·ðŸ»â€. Also the pruspose of this article is more about learning how to work with TensorFlow 2 Object Detection API rather than comming up with a production ready model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlkOLNbOsajd"
      },
      "source": [
        "## ðŸ¤·ðŸ»â€â™‚ï¸ The Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucfFZFofSSTx"
      },
      "source": [
        "I work as a software engineer and on my own time I learn Machine Learning as a hobby. But this is not the problem yet.\n",
        "\n",
        "I bought a printed book about Machine Learning recently and while I was reading through the first several chapters I've encountered many printed links in the text that looked like `https://tensorflow.org/` or `https://some-url.com/which/may/be/even/longer?and_with_params=true`.\n",
        "\n",
        "![Printed Links](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/02-printed-links.jpg)\n",
        "\n",
        "I saw all these links but I couldn't click on them since they were printed (thanks, cap!). To visit these links I needed to start typing them character by character in the browser's address bar, which was pretty annoying and error prone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj7Gc-KVs4q6"
      },
      "source": [
        "## ðŸ’¡ Possible Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob8k5gcfixDd"
      },
      "source": [
        "So, what if, similarly to QR-code detection, we will try to \"teach\" the smartphone to _(1)_ _detect_ and _(2)_ _recognize_ printed links for us and also to make them _clickable_? This way you'll do just one click instead of multiple keystrokes. The operational complexity goes from `O(N)` to `O(1)`.\n",
        "\n",
        "This is how the final workflow will look like:\n",
        "\n",
        "![Links Detector Demo](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/03-links-detector-demo.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfcnUFjcVDAy"
      },
      "source": [
        "## ðŸ“ Solution Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h2q_AQ_VMko"
      },
      "source": [
        "As I've mentioned earlier I'm just studying a Machine Learning as a hobby. Thus the pruspose of this article is more about _learning_ how to work with TensorFlow 2 Object Detection API rather than comming up with a production ready application.\n",
        "\n",
        "With that beign said, I simplified the solution requirements to the following:\n",
        "\n",
        "1. The detection and recognition processes should have a **close-to-real-time** performance (i.e. `0.5-1` frames per second) on a device like iPhone X. It means that whole _detection + recognition_ process should take up to `2` seconds (preatty bearable as for the amateur project).\n",
        "2. Only **English** links should be supported.\n",
        "3. Only **dark text** (i.e. black or dark-grey) on **light background** (i.e. white or light-grey) should be supported.\n",
        "4. Only `https://` links should be supported for now (it is ok if our model will not recognize the `http://`, `ftp://`, `tcp://` or other types of links)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4orlNJkZs7vB"
      },
      "source": [
        "## ðŸ§© Solution Breakdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VoRXizCNC-d"
      },
      "source": [
        "### High-level breakdown\n",
        "\n",
        "Let's see how we could approach the problem on a high level.\n",
        "\n",
        "#### Option 1: Detection model on the back-end\n",
        "\n",
        "**The flow:**\n",
        "\n",
        "1. Get camera stream (frame by frame) on the client side.\n",
        "2. Send each frame one by one over the network to the back-end.\n",
        "3. Do links detection and recognition on the back-end and send the response back to the client.\n",
        "4. Client draws the detection boxes with the clickable links.\n",
        "\n",
        "![Model on the back-end](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/04-frontend-backend.jpg)\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "- ðŸ’š The detection performance is not limited by the client's device. We may speed the detection up by scaling the service horizontally (adding more instances) and vertically (adding more cores/GPUs).\n",
        "- ðŸ’š The model might be bigger since there is no need to upload it to the client side. Downloading the `~10Mb` model on the client side may be ok, but loading the `~100Mb` model might be a big issue for the client's network and application UX (user experience) otherwise.\n",
        "- ðŸ’š It is possible to controll who is using the model. Model is guarded behind the API so we would have complete controll over its callers/clients.\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "- ðŸ’” System complexity growth. The aplication tech stack growth from just `JavaScript` to, let's say, `JavaScript + Python`. We need to take care about the autoscaling.\n",
        "- ðŸ’” Offline mode for the app is not possible since it needs an internet connection to work.\n",
        "- ðŸ’” Too many HTTP requests between the client and the server may become a bottleneck at some point. Imagine if we would want to improve the performance of the detecton, let's say, from `1` to `10+` frames per second. This means that each client will send `10+` requests per second. For `10` simultanious clients it is already `100+` requests per second. The `HTTP/2` bidirectional streaming and `gRPC` might be useful in this case, but we're going back to increased system complexity here.  \n",
        "- ðŸ’” System becomes more expensive. Almost all points from Pros section need to be paid for.\n",
        "\n",
        "#### Option 2: Detection model on the front-end\n",
        "\n",
        "**The flow:**\n",
        "\n",
        "1. Get camera stream (frame by frame) on the client side.\n",
        "2. Do links detection and recognition on the client side (without sending anything to the back-end).\n",
        "3. Client draws the detection boxes with the clickable links.\n",
        "\n",
        "![Model on the front-end](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/05-frontend-only.jpg)\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "- ðŸ’š System is less complex. We don't need to set up the servers, build the API and introcude an additional Python stack to the system. \n",
        "- ðŸ’š Offline mode is possible. The app doesn't need an internet connection to work since the model is loaded on the device. So the Progressive Web Application ([PWA](https://web.dev/progressive-web-apps/)) might be built to support that.\n",
        "- ðŸ’š System is \"kind of\" scaling automatically. The more clients you have, the more cores and GPUs they bring. This is not a proper scaling solution though (more about that in a Cons section below). \n",
        "- ðŸ’š System is cheaper. We only need a server for static assets (`HTML`, `JS`, `CSS`, model files etc.). This may be done for free, let's say, on GitHub.\n",
        "- ðŸ’š No issue with the growing number of HTTP requests per second to the server side.\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "- ðŸ’” Only the horizontal scaling is possible (each client will have it's own CPU/GPU). Vertical scaling is not possible since we can't influence the client's device performance. As a result we can't guarantee fast detection for low performant devices.\n",
        "- ðŸ’” It is not possible to guard the model usage and controll the callers/clients of the model. Everyone could download the model and re-use it. \n",
        "- ðŸ’” Battery consumption of the client's device might become an issue. For the model to work it needs computational resources. So clients might not be happy with their iPhone getting warmer and warmer while the app is working.\n",
        "\n",
        "#### High-level conslusion\n",
        "\n",
        "Since the purpose of the project was more about learning and not comming up with a production ready solution _I decided to go with the second option of serving the model from the client side_. This made the whole project much cheaper (actually with the GitHub it was free to host it) and I could focus more on Machine Learning then on the autoscaling back-end infrastructure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KQrN1ICEBpR"
      },
      "source": [
        "### Lower level breakdown\n",
        "\n",
        "Ok, so we've decided to go with the serverless solution. And now we have an image from the camera stream as an input that looks something like this:\n",
        "\n",
        "![Printed Links Input](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/06-printed-links-clean.jpg)\n",
        "\n",
        "We need to solve two sub-tasks for this image:\n",
        "\n",
        "1. Links **detection** (finding the position and bounding boxes of the links)\n",
        "2. Links **recognition** (recognizing the text of the links)\n",
        "\n",
        "#### Option 1: Tesseract based solution\n",
        "\n",
        "The first and the most obvious aproach would be to solve the _Optical Character Recognition_ ([OCR](https://en.wikipedia.org/wiki/Optical_character_recognition)) task by recognizing the whole text of the image by using, let's say, [Tesseract.js](https://github.com/naptha/tesseract.js) library. As a pleasent bonus it returns the bounding boxes of the paragraphs, text lines and text blocks along with the recognized text.\n",
        "\n",
        "![Recognized text with bounding boxes](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/07-printed-links-boxes.jpg)\n",
        "\n",
        "Then we may try to extract the links from the recognized text lines or text blocks with a regular expression like [this one](https://stackoverflow.com/questions/3809401/what-is-a-good-regular-expression-to-match-a-url):\n",
        "\n",
        "```typescript\n",
        "const URL_REG_EXP = /https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._+~#=]{2,256}\\.[a-z]{2,4}\\b([-a-zA-Z0-9@:%_+.~#?&/=]*)/gi;\n",
        "\n",
        "const extractLinkFromText = (text: string): string | null => {\n",
        "  const urls: string[] | null = text.match(URL_REG_EXP);\n",
        "  if (!urls || !urls.length) {\n",
        "    return null;\n",
        "  }\n",
        "  return urls[0];\n",
        "};\n",
        "```\n",
        "\n",
        "ðŸ’š Seems like the issue is solved in a pretty straightforward and simple way:\n",
        "\n",
        "- We know the bounding boxes of the links\n",
        "- And we also know the text of the links to make them clickable\n",
        "\n",
        "ðŸ’” The thing is that the _recognition + detection_ time may vary from `2` to `20+` seconds depending on the size of the text, on the ammount of \"something that looks like a text\" on the image, on the image quality and on other factors. So it will be realy hard to achive those `0.5-1` frames per second to make the user experience at least _close_ to the real-time.\n",
        "\n",
        "ðŸ’” Also if we would think about it, we're asking the library to recognize the **whole** text from the image for us even though it might contain only one or two links in it (i.e. only ~10% of the text might be usefull for us) or it may even not contain the links at all. In this case it sounds like a waste of the computational resources. \n",
        "\n",
        "#### Option 2: Tesseract + TensorFlow based solution\n",
        "\n",
        "We could make Tesseract work faster if we used some _additional \"adviser\" algorithm_ in prior to the links text recognition. This \"adviser\" algorithm should detect, but not recognize, _the the leftmost position_ of each link on the image if there are any. This will allow us to speed up the recognition part by following these rules:\n",
        "\n",
        "1. If the image does not contain any link we should not call Tesseract detection/recognition at all.\n",
        "2. If the image does have the links then we need to ask Tesseract to recognize only those parts of the image that contains the links. We're not interested in spending time for recognition of the irrelevant text that doesn't contain the links.\n",
        "\n",
        "The \"adviser\" algorithm that will take place before the Tesseract should work with a constant time regardless of the image quality or the presence/absence of the text on the image. It also should be pretty fast and detect the leftmost positions of the links for less then `1s` so that we could satisfy the \"close-to-real-time\" requirement (i.e. on iPhone X).\n",
        "\n",
        "> ðŸ’¡ So what if we will use another object detection model to help us find all occurrences of the `https://` substrings (every secure link has this prefix, doesn't it) in the image? Then, having these `https://` bounding boxes in the text we may extract the right-side continuation of them and send them to the Tesseract for text recognition.\n",
        "\n",
        "Take a look at the picture below:\n",
        "\n",
        "![Tesseract and TensorFlow based solution](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/08-tesseract-vs-tensorflow.jpg)\n",
        "\n",
        "You may notice that Tesseract needs to do **much less** work in case if it would have some hints about where are the links might be located (see the number of blue boxes on both pictures).\n",
        "\n",
        "So the question now is which object detection model we should choose and how to re-train it to support the detection of the custom `https://` objects.  \n",
        "\n",
        "> Finally! We've got closer to the TensorFlow part of the article ðŸ˜€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJTrZirnHEwf"
      },
      "source": [
        "## ðŸ¤– Selecting the object detection model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGT_-SxZHng-"
      },
      "source": [
        "Training a new object detection model is not a reasonable option in our context because of the following reasons:\n",
        "\n",
        "- ðŸ’” The training process might take days/weeks and bucks.\n",
        "- ðŸ’” We most probably won't be able to collect houndreds of thouthands of _labeled_ images of the books that have links in them (we might try to generate them though, but more about that later). \n",
        "\n",
        "So instead of creating a new model we should better teach an existing object detection model to do the custom object detection for us (to do the [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning)). In our case the \"custom objects\" would be the images with `https://` text drawn in them. This approach has the following benefits:\n",
        "\n",
        "- ðŸ’š The dataset might be much smaller. We don't need to collect houndreds of thouthands of the labeled images. Instead we may do `~100` pictures and label them manually. This is because the model is already pre-trained on the general dataset like [COCO dataset](https://cocodataset.org/#home) and already learned how to extract general image features.\n",
        "- ðŸ’š The training process will be much faster (minutes/hours on GPU instead of days/weeks). Again, this is because of a smaller dataset and because of fewer trainable parameters.\n",
        "\n",
        "We may choose the existing model from [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) which provides a collection of detection models pre-trained on the [COCO 2017 dataset](https://cocodataset.org/#home). For now it contains `~40` model variations to choose from.\n",
        "\n",
        "To re-train and fine-tune the model on the custom dataset we will use a [TensorFlow 2 Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection). The TensorFlow Object Detection API is an open source framework built on top of [TensorFlow](https://www.tensorflow.org/) that makes it easy to construct, train and deploy object detection models.\n",
        "\n",
        "If you follow the [Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) link you will find the _detection speed_ and _accuracy_ for each model.\n",
        "\n",
        "![Model Zoo](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/09-model-zoo.jpg)\n",
        "\n",
        "Of course we would want to find the right balance between the detection **speed** and **accuracy** while picking the model. But what might be even more important in our case is the **size** of the model since it will be loaded to the client side.\n",
        "\n",
        "The size of the archived model might vary drastically from `~20Mb` to `~1Gb`. Here are several examples:\n",
        "\n",
        "- `1386 (Mb)` `centernet_hg104_1024x1024_kpts_coco17_tpu-32`\n",
        "- ` 330 (Mb)` `centernet_resnet101_v1_fpn_512x512_coco17_tpu-8`\n",
        "- ` 195 (Mb)` `centernet_resnet50_v1_fpn_512x512_coco17_tpu-8`\n",
        "- ` 198 (Mb)` `centernet_resnet50_v1_fpn_512x512_kpts_coco17_tpu-8`\n",
        "- ` 227 (Mb)` `centernet_resnet50_v2_512x512_coco17_tpu-8`\n",
        "- ` 230 (Mb)` `centernet_resnet50_v2_512x512_kpts_coco17_tpu-8`\n",
        "- `  29 (Mb)` `efficientdet_d0_coco17_tpu-32`\n",
        "- `  49 (Mb)` `efficientdet_d1_coco17_tpu-32`\n",
        "- `  60 (Mb)` `efficientdet_d2_coco17_tpu-32`\n",
        "- `  89 (Mb)` `efficientdet_d3_coco17_tpu-32`\n",
        "- ` 151 (Mb)` `efficientdet_d4_coco17_tpu-32`\n",
        "- ` 244 (Mb)` `efficientdet_d5_coco17_tpu-32`\n",
        "- ` 376 (Mb)` `efficientdet_d6_coco17_tpu-32`\n",
        "- ` 376 (Mb)` `efficientdet_d7_coco17_tpu-32`\n",
        "- ` 665 (Mb)` `extremenet`\n",
        "- ` 427 (Mb)` `faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8`\n",
        "- ` 424 (Mb)` `faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8`\n",
        "- ` 337 (Mb)` `faster_rcnn_resnet101_v1_1024x1024_coco17_tpu-8`\n",
        "- ` 337 (Mb)` `faster_rcnn_resnet101_v1_640x640_coco17_tpu-8`\n",
        "- ` 343 (Mb)` `faster_rcnn_resnet101_v1_800x1333_coco17_gpu-8`\n",
        "- ` 449 (Mb)` `faster_rcnn_resnet152_v1_1024x1024_coco17_tpu-8`\n",
        "- ` 449 (Mb)` `faster_rcnn_resnet152_v1_640x640_coco17_tpu-8`\n",
        "- ` 454 (Mb)` `faster_rcnn_resnet152_v1_800x1333_coco17_gpu-8`\n",
        "- ` 202 (Mb)` `faster_rcnn_resnet50_v1_1024x1024_coco17_tpu-8`\n",
        "- ` 202 (Mb)` `faster_rcnn_resnet50_v1_640x640_coco17_tpu-8`\n",
        "- ` 207 (Mb)` `faster_rcnn_resnet50_v1_800x1333_coco17_gpu-8`\n",
        "- ` 462 (Mb)` `mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8`\n",
        "- `  86 (Mb)` `ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8`\n",
        "- `  44 (Mb)` `ssd_mobilenet_v2_320x320_coco17_tpu-8`\n",
        "- `  20 (Mb)` `ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8`\n",
        "- `  20 (Mb)` `ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8`\n",
        "- ` 369 (Mb)` `ssd_resnet101_v1_fpn_1024x1024_coco17_tpu-8`\n",
        "- ` 369 (Mb)` `ssd_resnet101_v1_fpn_640x640_coco17_tpu-8`\n",
        "- ` 481 (Mb)` `ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8`\n",
        "- ` 480 (Mb)` `ssd_resnet152_v1_fpn_640x640_coco17_tpu-8`\n",
        "- ` 233 (Mb)` `ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8`\n",
        "- ` 233 (Mb)` `ssd_resnet50_v1_fpn_640x640_coco17_tpu-8`\n",
        "\n",
        "The **`ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8`** model might be a good fit for our case:\n",
        "\n",
        "- ðŸ’š It is relativelly lightweight: `20Mb` archived.\n",
        "- ðŸ’š It is pretty fast: `39ms` for the detection.\n",
        "- ðŸ’š It uses MobileNet v2 network as a feature extractor which is optimized for usage on mobile devices and with lower energy consumption.\n",
        "- ðŸ’š It does the objects detection for the whole image and for the all objects in it **in one go** regardless of the image content. \n",
        "- ðŸ’” It is not the most accurate model though (everythin is a tradeof âš–ï¸).\n",
        "\n",
        "The model name encodes its several important characteristics that you may read more about if you want:\n",
        "\n",
        "- The expected image input size is `640x640px`.\n",
        "- The model implements [Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) (SSD) and [Feature Pyramid Network](https://arxiv.org/abs/1612.03144) (FPN).\n",
        "- [MobileNet v2](https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html) convolutional neural network ([CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network)) is used as a feature extractor.\n",
        "- The model was trained on [COCO dataset](https://cocodataset.org/#home)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsJzz0oPI3gM"
      },
      "source": [
        "##### Scripts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AiaPC8vfEW4",
        "outputId": "3ad63485-e177-400f-f408-3a581e7f1fcb"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "\n",
        "def _model_zoo():\n",
        "    CACHE_FOLDER = './models_zoo'\n",
        "    DATASETS_FOLDER = os.path.join(CACHE_FOLDER, 'datasets')\n",
        "\n",
        "    MODEL_NAMES = [\n",
        "    #   'centernet_hg104_512x512_kpts_coco17_tpu-3',\n",
        "    'centernet_hg104_1024x1024_kpts_coco17_tpu-32',\n",
        "    'centernet_resnet50_v1_fpn_512x512_coco17_tpu-8',\n",
        "    'centernet_resnet50_v1_fpn_512x512_kpts_coco17_tpu-8',\n",
        "    'centernet_resnet101_v1_fpn_512x512_coco17_tpu-8',\n",
        "    'centernet_resnet50_v2_512x512_coco17_tpu-8',\n",
        "    'centernet_resnet50_v2_512x512_kpts_coco17_tpu-8',\n",
        "    'efficientdet_d0_coco17_tpu-32',\n",
        "    'efficientdet_d1_coco17_tpu-32',\n",
        "    'efficientdet_d2_coco17_tpu-32',\n",
        "    'efficientdet_d3_coco17_tpu-32',\n",
        "    'efficientdet_d4_coco17_tpu-32',\n",
        "    'efficientdet_d5_coco17_tpu-32',\n",
        "    'efficientdet_d6_coco17_tpu-32',\n",
        "    'efficientdet_d7_coco17_tpu-32',\n",
        "    'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n",
        "    'ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8',\n",
        "    'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n",
        "    'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8',\n",
        "    'ssd_resnet50_v1_fpn_640x640_coco17_tpu-8',\n",
        "    'ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8',\n",
        "    'ssd_resnet101_v1_fpn_640x640_coco17_tpu-8',\n",
        "    'ssd_resnet101_v1_fpn_1024x1024_coco17_tpu-8',\n",
        "    'ssd_resnet152_v1_fpn_640x640_coco17_tpu-8',\n",
        "    'ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8',\n",
        "    'faster_rcnn_resnet50_v1_640x640_coco17_tpu-8',\n",
        "    'faster_rcnn_resnet50_v1_1024x1024_coco17_tpu-8',\n",
        "    'faster_rcnn_resnet50_v1_800x1333_coco17_gpu-8',\n",
        "    'faster_rcnn_resnet101_v1_640x640_coco17_tpu-8',\n",
        "    'faster_rcnn_resnet101_v1_1024x1024_coco17_tpu-8',\n",
        "    'faster_rcnn_resnet101_v1_800x1333_coco17_gpu-8',\n",
        "    'faster_rcnn_resnet152_v1_640x640_coco17_tpu-8',\n",
        "    'faster_rcnn_resnet152_v1_1024x1024_coco17_tpu-8',\n",
        "    'faster_rcnn_resnet152_v1_800x1333_coco17_gpu-8',\n",
        "    'faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8',\n",
        "    'faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8',\n",
        "    'mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8',\n",
        "    'extremenet',\n",
        "    ];\n",
        "\n",
        "    def create_cache(folder_name):\n",
        "        if not os.path.exists(folder_name):\n",
        "            os.makedirs(folder_name)\n",
        "\n",
        "    def download_tf_model(model_name, cache_path):\n",
        "        TF_MODELS_BASE_PATH = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'\n",
        "        model_url = TF_MODELS_BASE_PATH + model_name + '.tar.gz'\n",
        "        model_dir = tf.keras.utils.get_file(\n",
        "            fname=model_name, \n",
        "            origin=model_url,\n",
        "            untar=False,\n",
        "            cache_dir=pathlib.Path(cache_path).absolute()\n",
        "        )\n",
        "        return model_dir\n",
        "\n",
        "    def download_models(model_names, cache_path):\n",
        "        for model_name in model_names:\n",
        "            download_tf_model(model_name, cache_path)\n",
        "\n",
        "    def ls(base_dir):\n",
        "        KB = 1024\n",
        "        MB = KB * KB\n",
        "        names = os.listdir(base_dir)\n",
        "        sizes = [(name, os.stat(os.path.join(base_dir, name)).st_size) for name in sorted(names)]\n",
        "        for (name, size) in sizes:\n",
        "            size_mb = round(size / MB)\n",
        "            print('- `{:>4} (Mb)` `{}`'.format(size_mb, name))\n",
        "\n",
        "    create_cache(CACHE_FOLDER)\n",
        "    download_models(MODEL_NAMES, CACHE_FOLDER)\n",
        "    ls(DATASETS_FOLDER)\n",
        "\n",
        "_model_zoo()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- `1386 (Mb)` `centernet_hg104_1024x1024_kpts_coco17_tpu-32`\n",
            "- ` 330 (Mb)` `centernet_resnet101_v1_fpn_512x512_coco17_tpu-8`\n",
            "- ` 195 (Mb)` `centernet_resnet50_v1_fpn_512x512_coco17_tpu-8`\n",
            "- ` 198 (Mb)` `centernet_resnet50_v1_fpn_512x512_kpts_coco17_tpu-8`\n",
            "- ` 227 (Mb)` `centernet_resnet50_v2_512x512_coco17_tpu-8`\n",
            "- ` 230 (Mb)` `centernet_resnet50_v2_512x512_kpts_coco17_tpu-8`\n",
            "- `  29 (Mb)` `efficientdet_d0_coco17_tpu-32`\n",
            "- `  49 (Mb)` `efficientdet_d1_coco17_tpu-32`\n",
            "- `  60 (Mb)` `efficientdet_d2_coco17_tpu-32`\n",
            "- `  89 (Mb)` `efficientdet_d3_coco17_tpu-32`\n",
            "- ` 151 (Mb)` `efficientdet_d4_coco17_tpu-32`\n",
            "- ` 244 (Mb)` `efficientdet_d5_coco17_tpu-32`\n",
            "- ` 376 (Mb)` `efficientdet_d6_coco17_tpu-32`\n",
            "- ` 376 (Mb)` `efficientdet_d7_coco17_tpu-32`\n",
            "- ` 665 (Mb)` `extremenet`\n",
            "- ` 427 (Mb)` `faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8`\n",
            "- ` 424 (Mb)` `faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8`\n",
            "- ` 337 (Mb)` `faster_rcnn_resnet101_v1_1024x1024_coco17_tpu-8`\n",
            "- ` 337 (Mb)` `faster_rcnn_resnet101_v1_640x640_coco17_tpu-8`\n",
            "- ` 343 (Mb)` `faster_rcnn_resnet101_v1_800x1333_coco17_gpu-8`\n",
            "- ` 449 (Mb)` `faster_rcnn_resnet152_v1_1024x1024_coco17_tpu-8`\n",
            "- ` 449 (Mb)` `faster_rcnn_resnet152_v1_640x640_coco17_tpu-8`\n",
            "- ` 454 (Mb)` `faster_rcnn_resnet152_v1_800x1333_coco17_gpu-8`\n",
            "- ` 202 (Mb)` `faster_rcnn_resnet50_v1_1024x1024_coco17_tpu-8`\n",
            "- ` 202 (Mb)` `faster_rcnn_resnet50_v1_640x640_coco17_tpu-8`\n",
            "- ` 207 (Mb)` `faster_rcnn_resnet50_v1_800x1333_coco17_gpu-8`\n",
            "- ` 462 (Mb)` `mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8`\n",
            "- `  86 (Mb)` `ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8`\n",
            "- `  44 (Mb)` `ssd_mobilenet_v2_320x320_coco17_tpu-8`\n",
            "- `  20 (Mb)` `ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8`\n",
            "- `  20 (Mb)` `ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8`\n",
            "- ` 369 (Mb)` `ssd_resnet101_v1_fpn_1024x1024_coco17_tpu-8`\n",
            "- ` 369 (Mb)` `ssd_resnet101_v1_fpn_640x640_coco17_tpu-8`\n",
            "- ` 481 (Mb)` `ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8`\n",
            "- ` 480 (Mb)` `ssd_resnet152_v1_fpn_640x640_coco17_tpu-8`\n",
            "- ` 233 (Mb)` `ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8`\n",
            "- ` 233 (Mb)` `ssd_resnet50_v1_fpn_640x640_coco17_tpu-8`\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0t-B7net2gA"
      },
      "source": [
        "## ðŸ›  Installing Object Detection API "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMU2CRmXCAGm"
      },
      "source": [
        "In this article we're going to install the Tensorflow 2 Object Detection API _as a Python package_. It is convenient in case if you're experimenting in [Google Colab](https://colab.research.google.com/) or in [Jupyter](https://jupyter.org/try) (no local installation is needed, you may experiment right in your browser).\n",
        "\n",
        "You may also follow the [official documentation](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md) if you would preffer to install Object Detection API via Docker.\n",
        "\n",
        "First, let's clone the [API repository](https://github.com/tensorflow/models):\n",
        "\n",
        "```bash\n",
        "git clone --depth 1 https://github.com/tensorflow/models\n",
        "```\n",
        "\n",
        "_output â†’_\n",
        "\n",
        "```\n",
        "Cloning into 'models'...\n",
        "remote: Enumerating objects: 2301, done.\n",
        "remote: Counting objects: 100% (2301/2301), done.\n",
        "remote: Compressing objects: 100% (2000/2000), done.\n",
        "remote: Total 2301 (delta 561), reused 922 (delta 278), pack-reused 0\n",
        "Receiving objects: 100% (2301/2301), 30.60 MiB | 13.90 MiB/s, done.\n",
        "Resolving deltas: 100% (561/561), done.\n",
        "```\n",
        "\n",
        "Now, let's compile the [API proto files](https://github.com/tensorflow/models/tree/master/research/object_detection/protos) into Python files by using [protoc](https://grpc.io/docs/protoc-installation/) tool:\n",
        "\n",
        "```bash\n",
        "cd ./models/research\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "```\n",
        "\n",
        "And finally, let's install the TF2 version of [setup.py](https://github.com/tensorflow/models/blob/master/research/object_detection/packages/tf2/setup.py) via `pip`:\n",
        "\n",
        "```bash\n",
        "cp ./object_detection/packages/tf2/setup.py .\n",
        "pip install . --quiet\n",
        "```\n",
        "\n",
        "> It is possible that the last step will fail because of some dependency errors. In this case you might want to run `pip install . --quiet` one more time.\n",
        "\n",
        "We may test that installation went successfully by running the following tests:\n",
        "\n",
        "```bash\n",
        "python object_detection/builders/model_builder_tf2_test.py\n",
        "```\n",
        "\n",
        "You should see the logs that end with something similar to this:\n",
        "\n",
        "```\n",
        "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
        "----------------------------------------------------------------------\n",
        "Ran 20 tests in 45.072s\n",
        "\n",
        "OK (skipped=1)\n",
        "```\n",
        "\n",
        "The TensorFlow Object Detection API is installed! You may now use the scripts that API provides for doing the model [inference](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb), [training](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_training_and_evaluation.md) or [fine-tunning](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8x67CQmIaRG"
      },
      "source": [
        "##### Scripts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edimSsPB0eaJ",
        "outputId": "9d4b5463-9d38-46b7-e739-77d500a94b1e"
      },
      "source": [
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 2301, done.\u001b[K\n",
            "remote: Counting objects: 100% (2301/2301), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2000/2000), done.\u001b[K\n",
            "remote: Total 2301 (delta 561), reused 922 (delta 278), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2301/2301), 30.60 MiB | 13.90 MiB/s, done.\n",
            "Resolving deltas: 100% (561/561), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfJpqVKg1aIH"
      },
      "source": [
        "%%bash\n",
        "cd ./models/research\n",
        "# Compile protos.\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "# Install TensorFlow Object Detection API.\n",
        "cp ./object_detection/packages/tf2/setup.py .\n",
        "# In case if the following step will fail with dependency errors\n",
        "# try to launch it for the second time.\n",
        "pip install . --quiet"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0I3M293H6dB",
        "outputId": "bfdd2075-2c49-4069-f387-7a90d8d861de"
      },
      "source": [
        "%%bash\n",
        "cd ./models/research\n",
        "python object_detection/builders/model_builder_tf2_test.py"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-25 20:37:21.400630: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Running tests under Python 3.6.9: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model\n",
            "2020-11-25 20:37:25.041573: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-11-25 20:37:25.116208: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2020-11-25 20:37:25.116294: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (f2941dbc7ab0): /proc/driver/nvidia/version does not exist\n",
            "2020-11-25 20:37:25.237954: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\n",
            "2020-11-25 20:37:25.242280: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1590a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-25 20:37:25.242347: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model): 6.17s\n",
            "I1125 20:37:30.886950 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_center_net_model): 6.17s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "I1125 20:37:30.888568 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.04s\n",
            "I1125 20:37:30.929085 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.04s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n",
            "I1125 20:37:30.954617 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.03s\n",
            "I1125 20:37:30.980754 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.03s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.17s\n",
            "I1125 20:37:31.155953 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.17s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.16s\n",
            "I1125 20:37:31.316266 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.17s\n",
            "I1125 20:37:31.483444 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.17s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.16s\n",
            "I1125 20:37:31.648139 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.17s\n",
            "I1125 20:37:31.823006 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.17s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.05s\n",
            "I1125 20:37:31.873617 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.05s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I1125 20:37:32.210877 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I1125 20:37:32.211086 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 64\n",
            "I1125 20:37:32.211175 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 3\n",
            "I1125 20:37:32.219357 140155312019328 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1125 20:37:32.253079 140155312019328 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1125 20:37:32.253277 140155312019328 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1125 20:37:32.342541 140155312019328 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1125 20:37:32.342753 140155312019328 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1125 20:37:32.561972 140155312019328 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1125 20:37:32.562167 140155312019328 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1125 20:37:32.785304 140155312019328 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1125 20:37:32.785516 140155312019328 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1125 20:37:33.124763 140155312019328 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1125 20:37:33.125010 140155312019328 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1125 20:37:33.478866 140155312019328 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1125 20:37:33.479090 140155312019328 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1125 20:37:34.121132 140155312019328 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1125 20:37:34.121339 140155312019328 efficientnet_model.py:148] round_filter input=320 output=320\n",
            "I1125 20:37:34.249281 140155312019328 efficientnet_model.py:148] round_filter input=1280 output=1280\n",
            "I1125 20:37:34.317345 140155312019328 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1125 20:37:34.410243 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
            "I1125 20:37:34.410463 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 88\n",
            "I1125 20:37:34.410558 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 4\n",
            "I1125 20:37:34.417149 140155312019328 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1125 20:37:34.442621 140155312019328 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1125 20:37:34.442829 140155312019328 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1125 20:37:34.611623 140155312019328 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1125 20:37:34.611827 140155312019328 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1125 20:37:34.942299 140155312019328 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1125 20:37:34.942521 140155312019328 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1125 20:37:35.286432 140155312019328 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1125 20:37:35.286785 140155312019328 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1125 20:37:35.762995 140155312019328 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1125 20:37:35.763206 140155312019328 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1125 20:37:36.238858 140155312019328 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1125 20:37:36.239073 140155312019328 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1125 20:37:36.880821 140155312019328 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1125 20:37:36.881020 140155312019328 efficientnet_model.py:148] round_filter input=320 output=320\n",
            "I1125 20:37:37.159186 140155312019328 efficientnet_model.py:148] round_filter input=1280 output=1280\n",
            "I1125 20:37:37.230861 140155312019328 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1125 20:37:37.336688 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
            "I1125 20:37:37.336905 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 112\n",
            "I1125 20:37:37.336990 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 5\n",
            "I1125 20:37:37.343512 140155312019328 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1125 20:37:37.368550 140155312019328 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1125 20:37:37.368756 140155312019328 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1125 20:37:37.758982 140155312019328 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1125 20:37:37.759182 140155312019328 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1125 20:37:38.094413 140155312019328 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1125 20:37:38.094613 140155312019328 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1125 20:37:38.439531 140155312019328 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1125 20:37:38.439730 140155312019328 efficientnet_model.py:148] round_filter input=80 output=88\n",
            "I1125 20:37:38.916099 140155312019328 efficientnet_model.py:148] round_filter input=80 output=88\n",
            "I1125 20:37:38.916291 140155312019328 efficientnet_model.py:148] round_filter input=112 output=120\n",
            "I1125 20:37:39.421175 140155312019328 efficientnet_model.py:148] round_filter input=112 output=120\n",
            "I1125 20:37:39.421378 140155312019328 efficientnet_model.py:148] round_filter input=192 output=208\n",
            "I1125 20:37:40.098003 140155312019328 efficientnet_model.py:148] round_filter input=192 output=208\n",
            "I1125 20:37:40.098202 140155312019328 efficientnet_model.py:148] round_filter input=320 output=352\n",
            "I1125 20:37:40.397720 140155312019328 efficientnet_model.py:148] round_filter input=1280 output=1408\n",
            "I1125 20:37:40.467852 140155312019328 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1125 20:37:40.572979 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
            "I1125 20:37:40.573184 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 160\n",
            "I1125 20:37:40.573287 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 6\n",
            "I1125 20:37:40.579651 140155312019328 efficientnet_model.py:148] round_filter input=32 output=40\n",
            "I1125 20:37:40.604809 140155312019328 efficientnet_model.py:148] round_filter input=32 output=40\n",
            "I1125 20:37:40.605008 140155312019328 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1125 20:37:40.770252 140155312019328 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1125 20:37:40.770483 140155312019328 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1125 20:37:41.111239 140155312019328 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1125 20:37:41.111467 140155312019328 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1125 20:37:41.454507 140155312019328 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1125 20:37:41.454709 140155312019328 efficientnet_model.py:148] round_filter input=80 output=96\n",
            "I1125 20:37:42.035578 140155312019328 efficientnet_model.py:148] round_filter input=80 output=96\n",
            "I1125 20:37:42.035782 140155312019328 efficientnet_model.py:148] round_filter input=112 output=136\n",
            "I1125 20:37:42.924499 140155312019328 efficientnet_model.py:148] round_filter input=112 output=136\n",
            "I1125 20:37:42.924701 140155312019328 efficientnet_model.py:148] round_filter input=192 output=232\n",
            "I1125 20:37:43.739787 140155312019328 efficientnet_model.py:148] round_filter input=192 output=232\n",
            "I1125 20:37:43.740009 140155312019328 efficientnet_model.py:148] round_filter input=320 output=384\n",
            "I1125 20:37:44.042927 140155312019328 efficientnet_model.py:148] round_filter input=1280 output=1536\n",
            "I1125 20:37:44.128214 140155312019328 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1125 20:37:44.243096 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I1125 20:37:44.243314 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 224\n",
            "I1125 20:37:44.243422 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 7\n",
            "I1125 20:37:44.250236 140155312019328 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1125 20:37:44.276066 140155312019328 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1125 20:37:44.276265 140155312019328 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1125 20:37:44.448158 140155312019328 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1125 20:37:44.448367 140155312019328 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1125 20:37:44.900293 140155312019328 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1125 20:37:44.900521 140155312019328 efficientnet_model.py:148] round_filter input=40 output=56\n",
            "I1125 20:37:45.365758 140155312019328 efficientnet_model.py:148] round_filter input=40 output=56\n",
            "I1125 20:37:45.365966 140155312019328 efficientnet_model.py:148] round_filter input=80 output=112\n",
            "I1125 20:37:46.112512 140155312019328 efficientnet_model.py:148] round_filter input=80 output=112\n",
            "I1125 20:37:46.112710 140155312019328 efficientnet_model.py:148] round_filter input=112 output=160\n",
            "I1125 20:37:46.878419 140155312019328 efficientnet_model.py:148] round_filter input=112 output=160\n",
            "I1125 20:37:46.878624 140155312019328 efficientnet_model.py:148] round_filter input=192 output=272\n",
            "I1125 20:37:48.031826 140155312019328 efficientnet_model.py:148] round_filter input=192 output=272\n",
            "I1125 20:37:48.032033 140155312019328 efficientnet_model.py:148] round_filter input=320 output=448\n",
            "I1125 20:37:48.377604 140155312019328 efficientnet_model.py:148] round_filter input=1280 output=1792\n",
            "I1125 20:37:48.461341 140155312019328 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1125 20:37:48.596611 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I1125 20:37:48.596823 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 288\n",
            "I1125 20:37:48.596912 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 7\n",
            "I1125 20:37:48.603525 140155312019328 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1125 20:37:48.628953 140155312019328 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1125 20:37:48.629154 140155312019328 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1125 20:37:49.201881 140155312019328 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1125 20:37:49.202103 140155312019328 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1125 20:37:49.781805 140155312019328 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1125 20:37:49.782016 140155312019328 efficientnet_model.py:148] round_filter input=40 output=64\n",
            "I1125 20:37:50.366617 140155312019328 efficientnet_model.py:148] round_filter input=40 output=64\n",
            "I1125 20:37:50.366823 140155312019328 efficientnet_model.py:148] round_filter input=80 output=128\n",
            "I1125 20:37:51.235331 140155312019328 efficientnet_model.py:148] round_filter input=80 output=128\n",
            "I1125 20:37:51.235562 140155312019328 efficientnet_model.py:148] round_filter input=112 output=176\n",
            "I1125 20:37:52.129575 140155312019328 efficientnet_model.py:148] round_filter input=112 output=176\n",
            "I1125 20:37:52.129781 140155312019328 efficientnet_model.py:148] round_filter input=192 output=304\n",
            "I1125 20:37:53.513610 140155312019328 efficientnet_model.py:148] round_filter input=192 output=304\n",
            "I1125 20:37:53.513827 140155312019328 efficientnet_model.py:148] round_filter input=320 output=512\n",
            "I1125 20:37:54.119192 140155312019328 efficientnet_model.py:148] round_filter input=1280 output=2048\n",
            "I1125 20:37:54.210169 140155312019328 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1125 20:37:54.357953 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I1125 20:37:54.358172 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 384\n",
            "I1125 20:37:54.358261 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 8\n",
            "I1125 20:37:54.364747 140155312019328 efficientnet_model.py:148] round_filter input=32 output=56\n",
            "I1125 20:37:54.391062 140155312019328 efficientnet_model.py:148] round_filter input=32 output=56\n",
            "I1125 20:37:54.391267 140155312019328 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1125 20:37:54.661471 140155312019328 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1125 20:37:54.661674 140155312019328 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1125 20:37:55.363993 140155312019328 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1125 20:37:55.364216 140155312019328 efficientnet_model.py:148] round_filter input=40 output=72\n",
            "I1125 20:37:56.102076 140155312019328 efficientnet_model.py:148] round_filter input=40 output=72\n",
            "I1125 20:37:56.102278 140155312019328 efficientnet_model.py:148] round_filter input=80 output=144\n",
            "I1125 20:37:57.508346 140155312019328 efficientnet_model.py:148] round_filter input=80 output=144\n",
            "I1125 20:37:57.508578 140155312019328 efficientnet_model.py:148] round_filter input=112 output=200\n",
            "I1125 20:37:58.578772 140155312019328 efficientnet_model.py:148] round_filter input=112 output=200\n",
            "I1125 20:37:58.578974 140155312019328 efficientnet_model.py:148] round_filter input=192 output=344\n",
            "I1125 20:38:00.340754 140155312019328 efficientnet_model.py:148] round_filter input=192 output=344\n",
            "I1125 20:38:00.340959 140155312019328 efficientnet_model.py:148] round_filter input=320 output=576\n",
            "I1125 20:38:00.980234 140155312019328 efficientnet_model.py:148] round_filter input=1280 output=2304\n",
            "I1125 20:38:01.078814 140155312019328 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1125 20:38:01.249694 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
            "I1125 20:38:01.249898 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 384\n",
            "I1125 20:38:01.250004 140155312019328 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 8\n",
            "I1125 20:38:01.256485 140155312019328 efficientnet_model.py:148] round_filter input=32 output=64\n",
            "I1125 20:38:01.282324 140155312019328 efficientnet_model.py:148] round_filter input=32 output=64\n",
            "I1125 20:38:01.282528 140155312019328 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1125 20:38:01.631536 140155312019328 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1125 20:38:01.631732 140155312019328 efficientnet_model.py:148] round_filter input=24 output=48\n",
            "I1125 20:38:02.435065 140155312019328 efficientnet_model.py:148] round_filter input=24 output=48\n",
            "I1125 20:38:02.435271 140155312019328 efficientnet_model.py:148] round_filter input=40 output=80\n",
            "I1125 20:38:03.251243 140155312019328 efficientnet_model.py:148] round_filter input=40 output=80\n",
            "I1125 20:38:03.251573 140155312019328 efficientnet_model.py:148] round_filter input=80 output=160\n",
            "I1125 20:38:04.507314 140155312019328 efficientnet_model.py:148] round_filter input=80 output=160\n",
            "I1125 20:38:04.507538 140155312019328 efficientnet_model.py:148] round_filter input=112 output=224\n",
            "I1125 20:38:05.869168 140155312019328 efficientnet_model.py:148] round_filter input=112 output=224\n",
            "I1125 20:38:05.869383 140155312019328 efficientnet_model.py:148] round_filter input=192 output=384\n",
            "I1125 20:38:08.505453 140155312019328 efficientnet_model.py:148] round_filter input=192 output=384\n",
            "I1125 20:38:08.505649 140155312019328 efficientnet_model.py:148] round_filter input=320 output=640\n",
            "I1125 20:38:09.469557 140155312019328 efficientnet_model.py:148] round_filter input=1280 output=2560\n",
            "I1125 20:38:09.572362 140155312019328 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 37.89s\n",
            "I1125 20:38:09.768105 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 37.89s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "I1125 20:38:09.777465 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I1125 20:38:09.780162 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I1125 20:38:09.781145 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I1125 20:38:09.783176 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I1125 20:38:09.785145 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I1125 20:38:09.785872 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I1125 20:38:09.787174 140155312019328 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 20 tests in 45.072s\n",
            "\n",
            "OK (skipped=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nQV0aWtuFpV"
      },
      "source": [
        "## â¬‡ï¸ Downloading the Pre-Trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ASJmSwMLUu"
      },
      "source": [
        "Let's download our selected `ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8` model from the TensorFlow Model Zoo and check how it does the general objects detection (detection of the object classes from COCO dataset like \"cat\", \"dog\", \"car\", etc.).\n",
        "\n",
        "We will use the [get_file()](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file) TensorFlow helper to download the archived model from the URL and unpack it.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "\n",
        "MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'\n",
        "TF_MODELS_BASE_PATH = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'\n",
        "CACHE_FOLDER = './cache'\n",
        "\n",
        "def download_tf_model(model_name, cache_folder):\n",
        "    model_url = TF_MODELS_BASE_PATH + model_name + '.tar.gz'\n",
        "    model_dir = tf.keras.utils.get_file(\n",
        "        fname=model_name, \n",
        "        origin=model_url,\n",
        "        untar=True,\n",
        "        cache_dir=pathlib.Path(cache_folder).absolute()\n",
        "    )\n",
        "    return model_dir\n",
        "\n",
        "# Start the model download.\n",
        "model_dir = download_tf_model(MODEL_NAME, CACHE_FOLDER)\n",
        "print(model_dir)\n",
        "```\n",
        "\n",
        "_output â†’_\n",
        "\n",
        "```\n",
        "/content/cache/datasets/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\n",
        "```\n",
        "\n",
        "Here is how our folder structure looks so far:\n",
        "\n",
        "![Cache Folder](https://raw.githubusercontent.com/trekhleb/links-detector/master/articles/printed_links_detection/assets/10-cache-folder.jpg)\n",
        "\n",
        "The `checkpoint` folder contains the snapshot of pre-trained model.\n",
        "\n",
        "The `pipeline.config` file contains the detection settings of the model. We'll come back to this file later when we will need to fine-tune the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prjfd9CeM-w0"
      },
      "source": [
        "##### Scripts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ8zvG_ePtNS"
      },
      "source": [
        "mkdir -p ./cache"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fljmXp_eNAIa",
        "outputId": "d79245f2-78ed-466e-d427-e336c22d5692"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "\n",
        "MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'\n",
        "TF_MODELS_BASE_PATH = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'\n",
        "CACHE_FOLDER = './cache'\n",
        "\n",
        "def download_tf_model(model_name, cache_folder):\n",
        "    model_url = TF_MODELS_BASE_PATH + model_name + '.tar.gz'\n",
        "    model_dir = tf.keras.utils.get_file(\n",
        "        fname=model_name, \n",
        "        origin=model_url,\n",
        "        untar=True,\n",
        "        cache_dir=pathlib.Path(cache_folder).absolute()\n",
        "    )\n",
        "    return model_dir\n",
        "\n",
        "# Start the model downloading.\n",
        "model_dir = download_tf_model(MODEL_NAME, CACHE_FOLDER)\n",
        "print(model_dir)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cache/datasets/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaRq8YitST36",
        "outputId": "125ba1f7-d7a1-4401-aa5c-65ea218193dc"
      },
      "source": [
        "def get_folder_size(folder_path):\n",
        "    mB = 1000000\n",
        "    root_dir = pathlib.Path(folder_path)\n",
        "    sizeBytes = sum(f.stat().st_size for f in root_dir.glob('**/*') if f.is_file())\n",
        "    return f'{sizeBytes//mB} MB'\n",
        "\n",
        "print(f'Unpacked model size: {get_folder_size(model_dir)}')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unpacked model size: 31 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LpKaPIqPWph"
      },
      "source": [
        "## ðŸ„ðŸ»â€â™‚ï¸ Trying the Model (Inference)\n",
        "\n",
        "- Show that model works for general purpose classes\n",
        "- Show that model doesn't work for custom objects (links)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJQpRyWcud-r"
      },
      "source": [
        "## ðŸ“ Creating the Dataset Manually\n",
        "\n",
        "- Making pictures of the book\n",
        "- What tools to use to add bounding boxes\n",
        "- How to convert to protobuf\n",
        "- Issues with custom dataset (fonts, colors, bolds, underlined, etc.)\n",
        "- Train/test split approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H9ubdH7ZQqv"
      },
      "source": [
        "### ðŸŒ… Preprocessing the data\n",
        "\n",
        "- Data preprocessing: resize, crop square, color adjustment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_0iXNZPZWLz"
      },
      "source": [
        "### ðŸ”– Labeling the dataset\n",
        "\n",
        "- How to use LabelImg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKZgmaG6Zfbb"
      },
      "source": [
        "### ðŸ—œ Exporting the dataset\n",
        "\n",
        "- Protobuf (the way of storing the dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLAAnasyvBhS"
      },
      "source": [
        "## ðŸ“š Generating the Dataset Automatically (?)\n",
        "\n",
        "- Automated way of generating the dataset\n",
        "- Train/test split approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdk05ymSO7WK"
      },
      "source": [
        "## ðŸ“– Exploring the Dataset\n",
        "\n",
        "- Preview images with detection boxes\n",
        "- Number of images (why is this enough)\n",
        "- Do we need to preprocess the images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7triRLeP-_K"
      },
      "source": [
        "## ðŸ“ˆ Setting Up TensorBoard\n",
        "\n",
        "- Why do we need it (for debugging)\n",
        "- What we will monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhU74WMHQH1E"
      },
      "source": [
        "## ðŸ‘¨â€ðŸŽ“ Transfer Learning\n",
        "\n",
        "- What is transfer learning\n",
        "- Why don't we train the model from scratch\n",
        "- Allows us to use small dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7nFZjHUQR4A"
      },
      "source": [
        "### âš™ï¸ Configuring the Detection Pipeline\n",
        "\n",
        "- Performance issues: batch size\n",
        "- Starting not from scratch: checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lA_QoEhzQHCT"
      },
      "source": [
        "### ðŸ‹ðŸ»â€â™‚ï¸ Model Training\n",
        "\n",
        "- Error prone: saving checkpoints\n",
        "- How many epochs\n",
        "- Monitoring the performance while training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7OZozAHQklI"
      },
      "source": [
        "### ðŸš€ Evaluating the Model\n",
        "\n",
        "- Checking how accurate our model is on test dataset\n",
        "- Are we good with performance, should we save the model?\n",
        "- It is not a general purpose anymore, does it recognize our custom objects?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMiDAvqTQvg8"
      },
      "source": [
        "## ðŸ—œ Exporting the Model\n",
        "\n",
        "- Saving the model to the file for further re-use\n",
        "- Show the list of files, how the model looks like on dics\n",
        "- What the size of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlliMpvuQyUz"
      },
      "source": [
        "## ðŸš€ Evaluating the Exported Model\n",
        "\n",
        "- Example of how to use the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Juyv3FMkQ4qO"
      },
      "source": [
        "## ðŸ—œ Converting the Model for Web\n",
        "\n",
        "- What formats are sutable for the web\n",
        "- Few words about Tensorflow.js\n",
        "- Show list of exported files - how model looks like on disc\n",
        "- What the size of the model\n",
        "- Why it is split in chucnks and how they are connected (via model.json)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B8JU1UYzvBy",
        "outputId": "91b29075-0371-4018-e7a9-fbe08fc5055a"
      },
      "source": [
        "pip install tensorflowjs --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 10kB 26.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 20kB 12.7MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 30kB 9.5MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 51kB 4.7MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 61kB 5.3MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 4.1MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–                            | 10kB 23.4MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 20kB 20.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 30kB 16.4MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 40kB 14.5MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 51kB 11.1MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 61kB 11.1MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 71kB 7.4MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 81kB 8.1MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 92kB 8.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 102kB 8.2MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112kB 8.2MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhqXJkIpRF1A"
      },
      "source": [
        "## ðŸ¤” Conclusions\n",
        "\n",
        "- I'm just an amatour\n",
        "- Links to demo app\n",
        "- Issues and limitations of this approach\n",
        "- Links to my ML repositories that thy might like"
      ]
    }
  ]
}